{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder\n",
    "Model for encoding sentences into embedding vectors and then finding similarity between t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the Universal Sentence Encoder module from tensorflow hub.\n",
    "module_url = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
    "embed = hub.KerasLayer(module_url)\n",
    "\n",
    "#set of stop words to be removed while text pre-processing\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def decontracted(phrase):\n",
    "    \n",
    "    ''' method to de-contract the common phrases '''\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def striphtml(data): \n",
    "    ''' method to remove html tags from the text'''\n",
    "    cleanr = re.compile('<.*?>') \n",
    "    cleantext = re.sub(cleanr, ' ', str(data)) \n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def stripunc(data):\n",
    "    ''' method to remove punctuations and numbers'''\n",
    "    return re.sub('[^A-Za-z]+', ' ', str(data), flags=re.MULTILINE|re.DOTALL)\n",
    "\n",
    "\n",
    "def compute(sent): \n",
    "    '''Phrases in sentences are de-contracted, html tags and punctuations are removed and\n",
    "    finally words are brought back to their base form through lemmatization'''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    sent = decontracted(sent)   #decontracts the phrase\n",
    "    sent = striphtml(sent)      #strips html from the sentence\n",
    "    sent = stripunc(sent)       #removes the puncutations\n",
    "    words=word_tokenize(str(sent.lower())) #turns sentences in to lower case and then tokenizer it \n",
    "    words = [lemmatizer.lemmatize(word) for word in words] #lemmatization ie, changing words to their base form\n",
    "    words = [word for word in words if word not in stop_words] #removing stopwords\n",
    "    sent1 = ' '.join(words)\n",
    "    \n",
    "    return sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\Anuj\\\\OneDrive\\\\Desktop\\\\Precily_Assignment\\\\Text_Similarity_Dataset.csv')\n",
    "\n",
    "\n",
    "#apply text-preprocessing ie., removing stop words, stiping punctuations and html code and lemmatization\n",
    "df['text1'] = df['text1'].apply(lambda x: compute(x)) \n",
    "df['text2'] = df['text2'].apply(lambda x: compute(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searcher fail spot ad internet search en...</td>\n",
       "      <td>newcastle bolton kieron dyer smashed home winn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>million miss net uk population still without i...</td>\n",
       "      <td>nasdaq planning share sale owner technology do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short ginepri fifteen year old...</td>\n",
       "      <td>ruddock back yapp credential wale coach mike r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo buy u wine firm diageo world biggest sp...</td>\n",
       "      <td>mci share climb takeover bid share u phone com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>careful code new european directive could put ...</td>\n",
       "      <td>medium gadget get moving pocket sized device l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  savvy searcher fail spot ad internet search en...   \n",
       "1          1  million miss net uk population still without i...   \n",
       "2          2  young debut cut short ginepri fifteen year old...   \n",
       "3          3  diageo buy u wine firm diageo world biggest sp...   \n",
       "4          4  careful code new european directive could put ...   \n",
       "\n",
       "                                               text2  \n",
       "0  newcastle bolton kieron dyer smashed home winn...  \n",
       "1  nasdaq planning share sale owner technology do...  \n",
       "2  ruddock back yapp credential wale coach mike r...  \n",
       "3  mci share climb takeover bid share u phone com...  \n",
       "4  medium gadget get moving pocket sized device l...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    '''Method to calcualte cosine similarity betweeen two vectors'''\n",
    "    mag1 = np.linalg.norm(v1)\n",
    "    mag2 = np.linalg.norm(v2)\n",
    "    if (not mag1) or (not mag2):\n",
    "        return 0\n",
    "    return 1 - np.dot(v1, v2) / (mag1 * mag2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = []  #to hold the similarity between text1 and text2\n",
    "\n",
    "#iterate over the dataset and finds the similarity between every text in text1 and text2\n",
    "for i in range(0,len(df)):\n",
    "    #calling the cosine similarity function with one instance from text1 and text2\n",
    "    similarity.append( cosine_similarity(embed([df.iloc[i,1]])[0],   embed([df.iloc[i,2]])[0]) )\n",
    "    \n",
    "#saving the results back to the dataframe    \n",
    "df['similarity']=similarity\n",
    "df.to_csv('C:\\\\Users\\Anuj\\\\OneDrive\\\\Desktop\\\\Precily_Assignment\\\\Text_Similarity_Dataset_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5028485953807831, 0.32320380210876465, 0.30119407176971436, 0.32034367322921753, 0.22262650728225708, 0.2577289342880249, 0.46426230669021606, 0.17850518226623535, 0.2861446142196655, 0.10078173875808716]\n"
     ]
    }
   ],
   "source": [
    "print(similarity[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
